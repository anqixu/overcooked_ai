{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh9gVP2m4LKH"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "    %pip install pyyaml==5.1 filterpy imagesize tensorboard moviepy\n",
    "    %pip install --upgrade protobuf\n",
    "\n",
    "    # Install detectron2 that matches the above pytorch version\n",
    "    # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "    %pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
    "\n",
    "    exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1642740534302,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "jV9eik0Xc2H0",
    "outputId": "bd0ff8e2-06ad-4f39-fb1b-0e3be9503459"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4164,
     "status": "ok",
     "timestamp": 1642740541289,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "1Hf4w0bJ4FP4",
    "outputId": "1733d302-3e82-481e-fade-77f695f30086"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import moviepy as mpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.export.flatten import TracingAdapter\n",
    "from detectron2_backbone import *\n",
    "\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab.patches import cv2_imshow\n",
    "else:\n",
    "  def cv2_imshow(img):\n",
    "    plt.figure(figsize=(24, 16))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# WARNING: adjust maxsize if out of memory\n",
    "@functools.lru_cache(maxsize=400)\n",
    "def cv2_imread(img_path):\n",
    "  return cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17682,
     "status": "ok",
     "timestamp": 1642448664708,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "4N4DnTH9P33I",
    "outputId": "ce44b645-b777-42f0-ff58-b81f4af2c231"
   },
   "outputs": [],
   "source": [
    "# Setup colab environment\n",
    "\n",
    "if IN_COLAB:\n",
    "    ## Connect to GDrive, for ease of backup\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    os.makedirs(\"/content/drive/MyDrive/OvercookedColab/\", exist_ok=True)\n",
    "\n",
    "    # Download dataset\n",
    "    !wget https://www.dropbox.com/s/cu8zkb5bf2slqso/Overcooked2_1-1.zip?dl=1 -O Overcooked2_1-1.zip\n",
    "    !unzip Overcooked2_1-1.zip\n",
    "    !wget https://www.dropbox.com/s/8x5c004whvk646e/detection_dataset.json?dl=1 -O Overcooked2_1-1/detection_dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1642740690724,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "_29QR0A54FP7"
   },
   "outputs": [],
   "source": [
    "## Load dataset labels and split into train/val subsets\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from overcooked_ai.dataset_types import DetectionDataset\n",
    "\n",
    "if IN_COLAB:\n",
    "    SOURCE_DIR = Path(\"Overcooked2_1-1\")\n",
    "else:\n",
    "    SOURCE_DIR = Path(\"/home/mimic/Overcooked2_1-1_jpeg\")\n",
    "rng_seed = 1337\n",
    "training_subset_ratio = 0.75\n",
    "dataset_json = SOURCE_DIR / \"detection_dataset.mar2025.json\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = DetectionDataset.load_from_json(dataset_json)\n",
    "\n",
    "# Update file_name to absolute paths\n",
    "for entry in dataset.entries:\n",
    "    entry.file_name = str(SOURCE_DIR / entry.file_name)\n",
    "\n",
    "# Remove all segmentation annotations\n",
    "for entry in dataset.entries:\n",
    "    for anno in entry.annotations:\n",
    "        anno.segmentation_rle_counts = None\n",
    "\n",
    "dataset_dict = dataset.to_dict()[\"dataset_dict\"]\n",
    "thing_classes = dataset.thing_classes\n",
    "thing_colors = [(255, 255, 255) for _ in range(len(thing_classes))]\n",
    "\n",
    "# Split into train and val\n",
    "np.random.seed(rng_seed)\n",
    "np.random.shuffle(dataset_dict)\n",
    "num_training_entries = int(len(dataset_dict) * training_subset_ratio)\n",
    "train_dataset_dict = dataset_dict[:num_training_entries]\n",
    "val_dataset_dict = dataset_dict[num_training_entries:]\n",
    "\n",
    "# Register datasets into detectron2\n",
    "for d in [\"train\", \"val\"]:\n",
    "    dataset_name = \"overcooked_\" + d\n",
    "    if dataset_name in DatasetCatalog:\n",
    "        DatasetCatalog.remove(dataset_name)\n",
    "        MetadataCatalog.remove(dataset_name)\n",
    "    DatasetCatalog.register(dataset_name, lambda d=d: train_dataset_dict if d == \"train\" else val_dataset_dict)\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=thing_classes, thing_colors=thing_colors)\n",
    "overcooked_metadata = MetadataCatalog.get(\"overcooked_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3Y1Cqak4FP8"
   },
   "outputs": [],
   "source": [
    "## DEBUG: Visualize sample images and labels in training dataset\n",
    "\n",
    "for d in random.sample(train_dataset_dict, 2):\n",
    "    img = cv2_imread(d[\"file_name\"])\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=overcooked_metadata, scale=0.5, instance_mode=ColorMode.SEGMENTATION)\n",
    "    v._default_font_size = 40\n",
    "    out = v.draw_dataset_dict(d)\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1497466,
     "status": "ok",
     "timestamp": 1642456763048,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "hjFe7gRC4FP8",
    "outputId": "64648459-acc0-429b-a344-3990a8345c2f"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "# TODO: try training ViT-based\n",
    "\n",
    "# NOTE: this model needs gt_masks to be present in every single annotation in the dataset\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"overcooked_train\",)\n",
    "cfg.DATASETS.TEST = (\"overcooked_val\",)\n",
    "cfg.INPUT.RANDOM_FLIP = \"none\"\n",
    "cfg.MODEL.WEIGHTS = \"\"  #start with random initial weights\n",
    "cfg.SOLVER.BASE_LR = 0.0005\n",
    "cfg.SOLVER.MAX_ITER = 30000\n",
    "cfg.SOLVER.GAMMA = 0.5\n",
    "cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
    "cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "cfg.SOLVER.STEPS = [3000, 10000, 16000, 24000]  # update LR as GAMMA*LR at STEPS[i]\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[1.0,]]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.OFFSET = 0.5\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS = [0.75]\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 500\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.75\n",
    "cfg.SEED = rng_seed\n",
    "if IN_COLAB:\n",
    "    cfg.OUTPUT_DIR = os.path.join(\"overcooked_models\", datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "else:\n",
    "    cfg.OUTPUT_DIR = os.path.join(\"/home/mimic/objdet/overcooked_models\", datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "# (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736, 768, 800)\n",
    "cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1080\n",
    "cfg.INPUT.MIN_SIZE_TEST = 800\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1080\n",
    "cfg.INPUT.MASK_FORMAT = \"bitmask\"\n",
    "cfg.INPUT.RANDOM_FLIP = \"none\"\n",
    "\n",
    "# Free Colab settings\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "\n",
    "# Tuning for better convergence\n",
    "cfg.SOLVER.MAX_ITER = 200000\n",
    "cfg.SOLVER.STEPS = [3000, 6000, 10000, 14000, 20000, 25000, 30000, 50000, 75000]  # update LR as GAMMA*LR at STEPS[i]\n",
    "cfg.SOLVER.GAMMA = 0.75\n",
    "\n",
    "# TODO: try training at higher LR\n",
    "cfg.SOLVER.MAX_ITER = 100000\n",
    "cfg.SOLVER.STEPS = [5000, 10000, 15000, 25000, 50000]  # update LR as GAMMA*LR at STEPS[i]\n",
    "\n",
    "# Colab Pro settings  # NOTE: these almost double training time, but doesn't seem to improve convergence rate but rather actually slower\n",
    "# cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"model.yaml\"), \"w\") as fh:\n",
    "    fh.write(cfg.dump())\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"model.yaml\"), \"w\") as fh:\n",
    "    fh.write(cfg.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume previous training\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(\"/home/mimic/objdet/overcooked_models/20250323_180506/model.yaml\")\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"model.yaml\"), \"w\") as fh:\n",
    "    fh.write(cfg.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt = DifferentiableDLT()\n",
    "\n",
    "T_grid_xys = torch.tensor(corner_grid_xys, dtype=torch.float64).unsqueeze(0)\n",
    "T_frame_xys = torch.tensor(corner_frame_xys, dtype=torch.float64).unsqueeze(0)\n",
    "\n",
    "T_H_frame_grid = dlt(source_xys=T_grid_xys, target_xys=T_frame_xys)\n",
    "print(\"T_H_frame_grid\\n\", T_H_frame_grid[0])\n",
    "\n",
    "print(\"diff w/ gt:\")\n",
    "print(np.abs(T_H_frame_grid.cpu().detach().numpy() - H_frame_grid))\n",
    "\n",
    "T_pred_frame_xys = batch_apply_homography(T_H_frame_grid, T_grid_xys)\n",
    "print(\"T_pred_frame_xys\\n\", T_pred_frame_xys)\n",
    "print(\"diff w/ gt:\\n\", np.abs(T_pred_frame_xys.cpu().detach().numpy() - corner_frame_xys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXc7w-8N9OEg"
   },
   "outputs": [],
   "source": [
    "## Back up all content to Google Drive\n",
    "if IN_COLAB:\n",
    "    !rm overcooked_models/*/model_0*.pth\n",
    "    !du -h overcooked_models\n",
    "    !cp -r overcooked_models/* /content/drive/MyDrive/OvercookedColab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEEbmNqV9gZI"
   },
   "outputs": [],
   "source": [
    "# Save trained model to path\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    # files.download(os.path.join(cfg.OUTPUT_DIR, \"model_0014999.pth\"))\n",
    "    files.download(os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 698438,
     "status": "ok",
     "timestamp": 1641248549447,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "NLrc3eByWYmP",
    "outputId": "25746d08-bc20-4a8a-98b1-2ed38c60ef89"
   },
   "outputs": [],
   "source": [
    "# Upload previous checkpoint\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    content_dict = files.upload()\n",
    "    print(list(content_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuH8UZC789Rn"
   },
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir overcooked_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(os.path.join(\"/home/mimic/objdet/overcooked_models\", \"20250323_180506\", \"model.yaml\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKUWHpTO4FP9"
   },
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "eval_cfg = copy.deepcopy(cfg)\n",
    "eval_cfg.DATASETS.TEST = (\"overcooked_val\",)\n",
    "eval_cfg.MODEL.WEIGHTS = os.path.join(eval_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "eval_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15\n",
    "predictor = DefaultPredictor(eval_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2822,
     "status": "ok",
     "timestamp": 1642456976244,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "-tna_vza4FP9",
    "outputId": "8bb8188c-1dcc-4360-f1ae-514ebbe9ae37"
   },
   "outputs": [],
   "source": [
    "for d in random.sample(val_dataset_dict, 2):    \n",
    "    im = cv2_imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=overcooked_metadata,\n",
    "                   scale=0.5,\n",
    "                   instance_mode=ColorMode.SEGMENTATION \n",
    "    )\n",
    "    v._default_font_size = 27\n",
    "    output_instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    out = v.draw_instance_predictions(output_instances)\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393061,
     "status": "ok",
     "timestamp": 1642457396511,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "8NUasxxf4FP-",
    "outputId": "e5b98aa0-fea3-4408-f0e9-9be5576b7d22"
   },
   "outputs": [],
   "source": [
    "img_scale = 0.5\n",
    "eval_mp4_path = os.path.join(eval_cfg.OUTPUT_DIR, \"eval.mp4\")\n",
    "im = cv2_imread(os.path.join(SOURCE_DIR, dataset.entries[0].file_name))\n",
    "video_out = cv2.VideoWriter(eval_mp4_path, cv2.VideoWriter_fourcc(\"M\",\"P\",\"4\",\"V\"), 5, (int(im.shape[1] * img_scale), int(im.shape[0] * img_scale)))\n",
    "\n",
    "for entry in tqdm.tqdm(dataset.entries):\n",
    "    img_path = os.path.join(SOURCE_DIR, entry.file_name)\n",
    "    im = cv2_imread(img_path)\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=overcooked_metadata,\n",
    "                   scale=img_scale,\n",
    "                   instance_mode=ColorMode.SEGMENTATION\n",
    "    )\n",
    "    v._default_font_size = 27\n",
    "    output_instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    # output_instances = prune_annotations(output_instances, max_iou_overlap=0.3)  # TODO: fix this (pbly need to convert to BBoxAnnotation)\n",
    "    out = v.draw_instance_predictions(output_instances)\n",
    "    img_overlay = out.get_image()[:, :, ::-1]\n",
    "    video_out.write(img_overlay)\n",
    "video_out.release()\n",
    "print(f\"\\nWrote to {eval_mp4_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1641324324977,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "FxL_P6BU9Iw3",
    "outputId": "07684620-d424-40e6-db7b-183b9d4b619d"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(eval_mp4_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18799,
     "status": "ok",
     "timestamp": 1642457415279,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "EA7TOFi04FP-",
    "outputId": "68ee48e5-43c3-463a-ebb2-f80d4b644c39"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import pickle as pkl\n",
    "\n",
    "evaluator = COCOEvaluator(\"overcooked_val\", output_dir=os.path.join(eval_cfg.OUTPUT_DIR, \"eval\"))\n",
    "val_loader = build_detection_test_loader(eval_cfg, \"overcooked_val\")\n",
    "eval_results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "print(eval_results)\n",
    "with open(os.path.join(eval_cfg.OUTPUT_DIR, \"eval_results.pkl\"), \"wb\") as fh:\n",
    "  pkl.dump(eval_results, fh, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109752,
     "status": "ok",
     "timestamp": 1642457525180,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "CXUb_P36pC20",
    "outputId": "5d2d48ac-a793-4ce3-e869-dae5c6396be7"
   },
   "outputs": [],
   "source": [
    "# Load dataset without labels and insert predictions\n",
    "\n",
    "eval_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15\n",
    "predictor = DefaultPredictor(eval_cfg)\n",
    "\n",
    "foreground_categories = [\n",
    " 'chef',\n",
    " 'tuna',\n",
    " 'plate_cut_shrimp',\n",
    " 'plate_cut_tuna',\n",
    " 'shrimp',\n",
    " 'cut_shrimp',\n",
    " 'cut_tuna',\n",
    " 'icon_tuna',\n",
    " 'icon_shrimp',\n",
    "]\n",
    "foreground_category_ids = set(dataset.thing_classes.index(c) for c in foreground_categories)\n",
    "\n",
    "total_pred_annos = 0\n",
    "for entry_id, entry in enumerate(tqdm.tqdm(dataset.entries)):\n",
    "  # Wipe existing labels\n",
    "  entry.annotations = []\n",
    "\n",
    "  # Predict on image, then store predicted annotations\n",
    "  im = cv2_imread(os.path.join(SOURCE_DIR, entry.file_name))\n",
    "  outputs = predictor(im)\n",
    "  preds = outputs[\"instances\"].to(\"cpu\")\n",
    "  pred_bboxes = preds.pred_boxes.tensor.numpy()\n",
    "  pred_category_ids = preds.pred_classes.numpy()\n",
    "  scores = preds.scores.numpy()\n",
    "  for bbox, category_id, score in zip(pred_bboxes, pred_category_ids, scores):\n",
    "    # if category_id not in foreground_category_ids:\n",
    "    #   continue\n",
    "    anno = BBoxAnnotation(bbox[0], bbox[1], bbox[2], bbox[3], category_id, score=score)\n",
    "    entry.annotations.append(anno)\n",
    "    total_pred_annos += 1\n",
    "\n",
    "pred_dataset_json = os.path.join(eval_cfg.OUTPUT_DIR, \"detection_dataset.pred.json\")\n",
    "dataset.saveToJson(pred_dataset_json)\n",
    "print(f\"Wrote {total_pred_annos} total annotations to {pred_dataset_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4275,
     "status": "ok",
     "timestamp": 1642462296649,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "dgQFP_zyYBvG",
    "outputId": "ca47a32d-1787-4a21-c60a-d267c31ea8da"
   },
   "outputs": [],
   "source": [
    "# Remove overlapping annotations\n",
    "\n",
    "ds = DetectionDataset.loadFromJson(os.path.join(eval_cfg.OUTPUT_DIR, \"detection_dataset.pred.json\"))\n",
    "\n",
    "min_score = 0.175\n",
    "default_max_iou_overlap = 0.3\n",
    "category_max_iou_overlap = {\"chef\": 0.35}\n",
    "all_annos_max_iou_overlap = 0.8\n",
    "\n",
    "combo_categories_list = (\n",
    "    (\"ctop\", \"cut_shrimp\", \"cut_tuna\", \"plate_cut_shrimp\", \"plate_cut_tuna\"),\n",
    ")\n",
    "\n",
    "pruned_annos = 0\n",
    "kept_annos = 0\n",
    "for entry_idx, entry in enumerate(ds.entries):\n",
    "    # Count original annotations, then in the end subtract pruned annotations\n",
    "    kept_annos += len(entry.annotations)\n",
    "    seen_categories = set()\n",
    "\n",
    "    # Remove annotations with low scores\n",
    "    other_annos = []\n",
    "    for anno in entry.annotations:\n",
    "        if anno.score is not None and anno.score < min_score:\n",
    "            continue\n",
    "        other_annos.append(anno)\n",
    "    pruned_annos += len(entry.annotations) - len(other_annos)\n",
    "    entry.annotations = other_annos\n",
    "\n",
    "    # Group all annotations in each combo_categories tuple, and prune\n",
    "    for combo_categories in combo_categories_list:\n",
    "        for category in combo_categories:\n",
    "            assert(category not in seen_categories)\n",
    "            seen_categories.add(category)\n",
    "        target_annos = []\n",
    "        other_annos = []\n",
    "        for anno in entry.annotations:\n",
    "            if ds.thing_classes[anno.category_id] in combo_categories:\n",
    "                target_annos.append(anno)\n",
    "            else:\n",
    "                other_annos.append(anno)\n",
    "\n",
    "        max_iou_overlap = default_max_iou_overlap\n",
    "        for category in combo_categories:\n",
    "            if category in category_max_iou_overlap:\n",
    "                max_iou_overlap = category_max_iou_overlap[category]\n",
    "                break\n",
    "        kept_target_annos = prune_annotations(target_annos, max_iou_overlap)\n",
    "        entry.annotations = other_annos + kept_target_annos\n",
    "        pruned_annos += len(target_annos) - len(kept_target_annos)\n",
    "    \n",
    "    # Iterate over all other categories\n",
    "    for category in ds.thing_classes:\n",
    "        if category in seen_categories:\n",
    "            continue\n",
    "        seen_categories.add(category)\n",
    "        target_category_id = ds.thing_classes.index(category)\n",
    "        target_annos = []\n",
    "        other_annos = []\n",
    "        for anno in entry.annotations:\n",
    "            if anno.category_id == target_category_id:\n",
    "                target_annos.append(anno)\n",
    "            else:\n",
    "                other_annos.append(anno)\n",
    "\n",
    "        max_iou_overlap = category_max_iou_overlap[category] if category in category_max_iou_overlap else default_max_iou_overlap\n",
    "        kept_target_annos = prune_annotations(target_annos, max_iou_overlap)\n",
    "        entry.annotations = other_annos + kept_target_annos\n",
    "        pruned_annos += len(target_annos) - len(kept_target_annos)\n",
    "    \n",
    "    # Iterate over all categories\n",
    "    target_annos = entry.annotations\n",
    "    entry.annotations = prune_annotations(target_annos, all_annos_max_iou_overlap)\n",
    "    pruned_annos += len(target_annos) - len(entry.annotations)\n",
    "\n",
    "kept_annos -= pruned_annos\n",
    "\n",
    "print(f\"Pruned {pruned_annos} and kept {kept_annos} annotations\")\n",
    "\n",
    "ds.saveToJson(os.path.join(eval_cfg.OUTPUT_DIR, \"detection_dataset.pred_2.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJvw2v9mp9TG"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(pred_dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1719,
     "status": "ok",
     "timestamp": 1642457526875,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "Lrsm8DUqOrW5",
    "outputId": "4346ab1c-909f-44d6-c249-31fb7e53c48c"
   },
   "outputs": [],
   "source": [
    "## Back up all content to Google Drive\n",
    "if IN_COLAB:\n",
    "    !rm overcooked_models/*/model_0*.pth\n",
    "    !du -h overcooked_models\n",
    "    !cp -r overcooked_models/* /content/drive/MyDrive/OvercookedColab/\n",
    "else:\n",
    "    # TODO: test this logic\n",
    "    CKPT_DIR = eval_cfg.OUTPUT_DIR\n",
    "    !rm $CKPT_DIR/model_0*.pth\n",
    "    !cp -r $CKPT_DIR /home/mimic/GDrive/OvercookedColab/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAO4C9Ck37dD"
   },
   "outputs": [],
   "source": [
    "## TEST: Export model via tracing\n",
    "from detectron2.utils.testing import assert_instances_allclose\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "model = predictor.model.to(\"cuda\")\n",
    "exported_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_exported.pt\")\n",
    "# exported_model_path = \"model_exported.pt\"\n",
    "\n",
    "dataset = DetectionDataset.loadFromJson(dataset_json)\n",
    "\n",
    "# Load sample image\n",
    "image = cv2_imread(os.path.join(SOURCE_DIR, dataset.entries[0].file_name))\n",
    "image = torch.from_numpy(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "inputs = tuple(image.clone() for _ in range(batch_size))\n",
    "\n",
    "def inference_func(model, image):\n",
    "  inputs = [{\"image\": image}]\n",
    "  return model.inference(inputs, do_postprocess=False)[0]\n",
    "\n",
    "wrapper = TracingAdapter(model, inputs, inference_func)\n",
    "wrapper.eval()\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(wrapper, inputs)\n",
    "    outputs = inference_func(model, *inputs)\n",
    "    traced_outputs = wrapper.outputs_schema(traced_model(*inputs))\n",
    "\n",
    "    if batch_size > 1:\n",
    "        for output, traced_output in zip(outputs, traced_outputs):\n",
    "            assert_instances_allclose(output, traced_output, size_as_tensor=True)\n",
    "    else:\n",
    "        assert_instances_allclose(outputs, traced_outputs, size_as_tensor=True)\n",
    "    print(\"Successfully exported traced model\")\n",
    "  \n",
    "    traced_model.save(exported_model_path)\n",
    "    print(f\"Saved to {exported_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5579,
     "status": "ok",
     "timestamp": 1641139997230,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "Ivwxlpt8Xnz5",
    "outputId": "5ad3cf2e-6353-4d6e-d673-c5845a9b6a91"
   },
   "outputs": [],
   "source": [
    "## TEST: Inference speed of traced model\n",
    "\n",
    "device = \"cuda\"\n",
    "loaded_model = torch.jit.load(exported_model_path)\n",
    "\n",
    "# image = cv2.imread(os.path.join(SOURCE_DIR, dataset.entries[0].file_name))\n",
    "# image = torch.from_numpy(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "\n",
    "dts = []\n",
    "for idx in range(10):\n",
    "  tic = time.time()\n",
    "  with torch.no_grad():\n",
    "    pred_boxes, pred_classes, scores, height_width = loaded_model(image)\n",
    "  toc = time.time()\n",
    "  if idx > 1:\n",
    "    dts.append(toc - tic)\n",
    "print(f\"Did {len(dts)} inferences, avg {np.mean(dts):.3f} sec (std: {np.std(dts):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1641004333814,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "8g3r0OE5csZ0",
    "outputId": "dae000b0-43ca-4f62-e0fd-e10ad36bf318"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(exported_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1641139989892,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "ogbDfBthhDs4",
    "outputId": "1c1d7753-7064-4615-8dbc-3e512edc4213"
   },
   "outputs": [],
   "source": [
    "## TEST: inference speed of detectron2 model\n",
    "\n",
    "im = cv2_imread(\"frame_20210703_162259.789.png\")\n",
    "dts = []\n",
    "for _ in range(10):\n",
    "  tic = time.time()\n",
    "  outputs = predictor(im)\n",
    "  preds = outputs[\"instances\"].to(\"cpu\")\n",
    "  toc = time.time()\n",
    "  dts.append(toc - tic)\n",
    "print(f\"Did {len(dts)} inferences, avg {np.mean(dts):.3f} sec (std: {np.std(dts):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYni7NhukvxM"
   },
   "outputs": [],
   "source": [
    "## TEST: create a Module with backbone as additional output, and trace it\n",
    "\n",
    "import collections\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from torch import nn\n",
    "\n",
    "model = predictor.model\n",
    "model.eval()\n",
    "im = cv2_imread(\"frame_20210703_162259.789.png\")\n",
    "image_tensor = torch.from_numpy(np.ascontiguousarray(im.transpose(2, 0, 1)))\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    Out = collections.namedtuple(\"Out\", (\"pred_boxes\", \"scores\", \"pred_classes\", \"fpn_p2\"))\n",
    "\n",
    "    def __init__(self, rcnn):\n",
    "        super().__init__()\n",
    "        self.rcnn = rcnn\n",
    "\n",
    "    def forward(self, x):\n",
    "        images = self.rcnn.preprocess_image([{\"image\": x}])\n",
    "        features = self.rcnn.backbone(images.tensor)\n",
    "        proposals, _ = self.rcnn.proposal_generator(images, features)\n",
    "        instances, _ = self.rcnn.roi_heads(images, features, proposals)\n",
    "        assert(len(instances) == 1)\n",
    "        return CustomModel.Out(\n",
    "            pred_boxes=instances[0].pred_boxes.tensor,\n",
    "            scores=instances[0].scores,\n",
    "            pred_classes=instances[0].pred_classes,\n",
    "            fpn_p2=features[\"p2\"]\n",
    "        )\n",
    "\n",
    "cmodel = CustomModel(model)\n",
    "with torch.no_grad():\n",
    "  output = cmodel(image_tensor)\n",
    "  traced_cmodel = torch.jit.trace(cmodel, image_tensor)\n",
    "  traced_output = traced_cmodel(image_tensor)\n",
    "  assert(np.all([torch.all(output.__getattribute__(k).eq(traced_output[idx])).cpu() for idx, k in enumerate(output._fields)])), \"traced model output mismatches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 1865,
     "status": "error",
     "timestamp": 1641327373993,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "iajVxsxkzhk_",
    "outputId": "303c4799-1726-4f5b-fc35-64d35d6a7c0f"
   },
   "outputs": [],
   "source": [
    "######\n",
    "######\n",
    "# HOMOGRAPHY REGRESSOR MODEL\n",
    "######\n",
    "######\n",
    "\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from detectron2.modeling import build_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def vec2H(entry_H_grid_img):\n",
    "    H_grid_img = np.array(entry_H_grid_img).reshape(3, 3)\n",
    "    H_grid_img /= H_grid_img[2, 2]\n",
    "    return H_grid_img\n",
    "\n",
    "\n",
    "def compute_whiten_H_vec(dataset):\n",
    "    \"\"\"\n",
    "    Usage: H_vec_whitened = (H_vec + H_vec_bias) * H_vec_scale\n",
    "           H_vec = H_vec_whitened / H_vec_scale - H_vec_bias\n",
    "    \"\"\"\n",
    "    Hs = np.zeros((len(dataset.entries), 8))\n",
    "    for idx, entry in enumerate(dataset.entries):\n",
    "        H_grid_img = vec2H(entry.H_grid_img)\n",
    "        Hs[idx, :] = H_grid_img.flatten()[:8]\n",
    "    H_vec_bias = -np.mean(Hs, axis=0)\n",
    "    H_vec_scale = 1/np.std(Hs, axis=0)\n",
    "    return H_vec_bias, H_vec_scale\n",
    "\n",
    "\n",
    "class ImageHomographyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, detection_dataset, is_train, transform=None):\n",
    "        self.transform = transform\n",
    "        self.ds_all = detection_dataset\n",
    "        \n",
    "        self.ds = copy.deepcopy(self.ds_all)\n",
    "        half_image_idx = int(len(self.ds_all.entries)//2)\n",
    "        if is_train:\n",
    "          self.ds.entries = self.ds.entries[:half_image_idx]\n",
    "        else:\n",
    "          self.ds.entries = self.ds.entries[half_image_idx:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds.entries)\n",
    "    \n",
    "    def __getitem__(self, entry_idx):\n",
    "        entry = self.ds.entries[entry_idx]\n",
    "        img_bgr = cv2_imread(os.path.join(SOURCE_DIR, entry.file_name))\n",
    "        sample = {\n",
    "            \"image\": img_bgr,\n",
    "            \"H_grid_img\": vec2H(entry.H_grid_img),\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "# TODO: test if this works and makes sense; remember to keep source image size in order to be able to batch\n",
    "class RandomTranslate(object):\n",
    "    \"\"\"\n",
    "    Uniformly sample dx/dy, then crop a shifted version of the input image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dx_minmax=(0, 100), dy_minmax=(0, 100)):\n",
    "        self.dx_minmax = dx_minmax\n",
    "        self.dy_minmax = dy_minmax\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        dx = int(np.random.uniform(*self.dx_minmax))\n",
    "        dy = int(np.random.uniform(*self.dy_minmax))\n",
    "        source_image = sample[\"image\"]\n",
    "        cropped_image = source_image[dy:, dx:]\n",
    "        shifted_image = np.zeros(source_image.shape, source_image.dtype)\n",
    "        shifted_image[:cropped_image.shape[0], :cropped_image.shape[1]] = cropped_image\n",
    "        shifted_sample = {\n",
    "            \"image\": shifted_image,\n",
    "            \"H_grid_img\": sample[\"H_grid_img\"] @ np.array(((1, 0, dx), (0, 1, dy), (0, 0, 1))),\n",
    "        }\n",
    "        return shifted_sample\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        tensor_sample = {\n",
    "            \"image\": torch.from_numpy(np.ascontiguousarray(sample[\"image\"].transpose(2, 0, 1))),\n",
    "            \"H_vec\": torch.from_numpy(sample[\"H_vec\"].astype(\"float32\"))\n",
    "        }\n",
    "        return tensor_sample\n",
    "\n",
    "class HomographyRegressor(torch.nn.Module):\n",
    "    def __init__(self, rcnn, image_width_height=(1920,1080), global_pooling=False, fpn_p_num_channels=256, fc_size=256):\n",
    "        super().__init__()\n",
    "        self.global_pooling = global_pooling\n",
    "        self.preprocess_image = rcnn.preprocess_image\n",
    "        self.backbone = rcnn.backbone\n",
    "        self.backbone.require_grad = False\n",
    "        self.device = rcnn.device\n",
    "        input_size = fpn_p_num_channels*5 if global_pooling else int(fpn_p_num_channels*image_width_height[0]*image_width_height[1]*(1/64)*(1/64))\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(input_size, fc_size),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(fc_size, fc_size),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(fc_size, 8),\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        images = self.preprocess_image(x)\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        fpn_p2_features = features[\"p2\"]\n",
    "        fpn_p3_features = features[\"p3\"]\n",
    "        fpn_p4_features = features[\"p4\"]\n",
    "        fpn_p5_features = features[\"p5\"]\n",
    "        fpn_p6_features = features[\"p6\"]\n",
    "        if self.global_pooling:\n",
    "            feature_vec = torch.cat((\n",
    "                fpn_p2_features.mean([2, 3]),\n",
    "                fpn_p3_features.mean([2, 3]),\n",
    "                fpn_p4_features.mean([2, 3]),\n",
    "                fpn_p5_features.mean([2, 3]),\n",
    "                fpn_p6_features.mean([2, 3]),\n",
    "            ), dim=1)\n",
    "        else:\n",
    "            num_samples = len(x)\n",
    "            feature_vec = torch.cat((\n",
    "                fpn_p6_features.reshape((num_samples, -1)),\n",
    "            ), dim=1)\n",
    "        H_vec = self.regressor(feature_vec)\n",
    "\n",
    "        return H_vec\n",
    "\n",
    "#####\n",
    "\n",
    "dataset = DetectionDataset.loadFromJson(dataset_json)\n",
    "\n",
    "H_vec_bias, H_vec_scale = compute_whiten_H_vec(dataset)\n",
    "class FlattenWhitenHomography(object):\n",
    "    def __call__(self, sample):\n",
    "        H = sample[\"H_grid_img\"]\n",
    "        sample[\"H_vec\"] = (H.flatten()[:8] + H_vec_bias) * H_vec_scale\n",
    "        return sample\n",
    "\n",
    "train_dataset = ImageHomographyDataset(dataset, is_train=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        # RandomTranslate(),  # TODO: experiment\n",
    "        FlattenWhitenHomography(),\n",
    "        ToTensor(),\n",
    "    ]))\n",
    "\n",
    "test_dataset = ImageHomographyDataset(dataset, is_train=False,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        FlattenWhitenHomography(),\n",
    "        ToTensor(),\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2, num_workers=1, collate_fn=list)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=2, num_workers=1, collate_fn=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qfPbl2uumgm"
   },
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))\n",
    "images = detectron2_model.preprocess_image(x)\n",
    "features = detectron2_model.backbone(images.tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBIIO6xq8B1y"
   },
   "outputs": [],
   "source": [
    "detectron2_cfg = get_cfg()\n",
    "# detectron2_cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "# detectron2_cfg.MODEL.WEIGHTS = \"overcooked_models/20220103_225637/model_final.pth\"\n",
    "# detectron2_cfg.MODEL.WEIGHTS = os.path.join(eval_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "detectron2_cfg.merge_from_file(\"overcooked_models/20220104_163857/model.yaml\")\n",
    "\n",
    "detectron2_model = build_model(detectron2_cfg)\n",
    "detectron2_model.eval()\n",
    "\n",
    "model = HomographyRegressor(detectron2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vNYy7XMF9Lj"
   },
   "outputs": [],
   "source": [
    "class HomographyRegressorTrainer(object):\n",
    "    CKPT_NAME = \"homography_regressor.pt\"\n",
    "    \n",
    "    def __init__(self, model, log_dir, lr=4e-5, device_name=\"cuda:0\"):\n",
    "        self.log_dir = log_dir\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "            \n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.device = torch.device(device_name if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        self.optimizer = torch.optim.Adam(self.model.regressor.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5000, gamma=0.5)\n",
    "        \n",
    "        self.iter_idx = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(log_dir, device_name=\"cuda:0\"):\n",
    "        raise NotImplementedError()\n",
    "        # ckpt_path = os.path.join(log_dir, HomographyRegressorTrainer.CKPT_NAME)\n",
    "        # checkpoint = torch.load(ckpt_path)\n",
    "        # model = GOTURN(load_from_imagenet=False)\n",
    "        # trainer = GOTURNTrainer(model, log_dir, device_name=device_name)\n",
    "        # trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        # trainer.iter_idx = checkpoint['iter_idx']\n",
    "        # return trainer\n",
    "    \n",
    "    def save(self, ckpt_path=None):\n",
    "        return\n",
    "        # raise NotImplementedError()\n",
    "        # if ckpt_path is None:\n",
    "        #     ckpt_path = os.path.join(self.log_dir, HomographyRegressorTrainer.CKPT_NAME)\n",
    "        # self.flush_writer()\n",
    "        # torch.save({\n",
    "        #     'iter_idx': self.iter_idx,\n",
    "        #     'model_state_dict': self.model.state_dict(),\n",
    "        #     'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        #     'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "        #     }, ckpt_path)\n",
    "        \n",
    "    def flush_writer(self):\n",
    "        self.writer.get_file_writer().flush()\n",
    "        \n",
    "    def train(self, loader, num_iters=50000, evalloader=None, eval_every_n_iters=5000, save_every_n_iters=1000):\n",
    "        self.model.train()\n",
    "        latest_loss = -1\n",
    "        latest_eval_loss = -1\n",
    "        iter_pbar = tqdm.tqdm_notebook(total=num_iters, initial=self.iter_idx, desc=\"train\")\n",
    "        losses = []\n",
    "        while self.iter_idx < num_iters:\n",
    "            for batch in loader:\n",
    "                gt_H_vec = torch.stack(tuple(entry[\"H_vec\"] for entry in batch)).to(self.device)\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                pred_H_vec = self.model(batch)\n",
    "                loss = self.criterion(pred_H_vec, gt_H_vec)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                latest_loss = loss.item()\n",
    "                losses.append(latest_loss)\n",
    "                lr = self.scheduler.get_last_lr()\n",
    "                iter_pbar.update()\n",
    "                iter_pbar.set_postfix(a=lr, eL=latest_eval_loss, tL=latest_loss)\n",
    "                self.writer.add_scalar(\"train/lr\", lr, self.iter_idx)\n",
    "                self.writer.add_scalar(\"train/loss\", latest_loss, self.iter_idx)\n",
    "\n",
    "                self.iter_idx += 1\n",
    "                if eval_every_n_iters > 0 and evalloader is not None and self.iter_idx % eval_every_n_iters == 0:\n",
    "                    latest_eval_loss = self.eval(evalloader)[\"avg_L2\"]\n",
    "                iter_pbar.set_postfix(a=lr, eL=latest_eval_loss, tL=latest_loss)\n",
    "                if save_every_n_iters > 0 and self.iter_idx % save_every_n_iters == 0:\n",
    "                    self.save()\n",
    "                if self.iter_idx >= num_iters:\n",
    "                    iter_pbar.close()\n",
    "                    break\n",
    "        self.flush_writer()\n",
    "        return losses\n",
    "\n",
    "    def eval(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        entry_count = 0\n",
    "        batch_pbar = tqdm.tqdm_notebook(loader, desc=\"eval\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(batch_pbar):\n",
    "                gt_H_vec = torch.stack(tuple(entry[\"H_vec\"] for entry in batch)).to(self.device)\n",
    "                pred_H_vec = self.model(batch)\n",
    "                loss = self.criterion(pred_H_vec, gt_H_vec)\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                entry_count += len(batch)\n",
    "                batch_pbar.set_postfix(l=total_loss/batch_count)\n",
    "        latest_loss = total_loss/batch_count\n",
    "        stats = {\n",
    "            \"avg_L2\": latest_loss,\n",
    "        }\n",
    "        self.writer.add_scalar('eval/loss', latest_loss, self.iter_idx)\n",
    "        return stats\n",
    "\n",
    "log_dir = os.path.join(\"homography_regressor_models\", datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "shutil.rmtree(log_dir, ignore_errors=True)\n",
    "os.makedirs(log_dir)\n",
    "trainer = HomographyRegressorTrainer(model=model, log_dir=log_dir, lr=8e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmZPhUkx82FF"
   },
   "outputs": [],
   "source": [
    "losses = trainer.train(train_loader, num_iters=50000, evalloader=test_loader, eval_every_n_iters=5000)\n",
    "# TODO: debug why it's taking ~1/1.23 sec per iteration. Forward inference through backbone should be around 0.1 sec. Maybe batch loader? Maybe FCNet? Maybe some other inefficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5139,
     "status": "ok",
     "timestamp": 1642433078424,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "GGXlRTA_p2B7",
    "outputId": "3e2abe23-17c3-42cc-b9f0-2259c1dbf224"
   },
   "outputs": [],
   "source": [
    "### ANALYZE: load dataset and track contiguous frames based on category and iou+dist proximity\n",
    "\n",
    "ds = DetectionDataset.loadFromJson(os.path.join(SOURCE_DIR, \"detection_dataset.json\"))\n",
    "\n",
    "tracks = [[] for _ in range(len(ds.thing_classes))]  # tracks[category_id] = list(list(entry_idx, anno_idx, bbox_xyxy), ...)\n",
    "active_tracks = [[] for _ in range(len(ds.thing_classes))]\n",
    "iou_threshs = {ds.thing_classes.index(category): 0.1 for category in (\"icon_plate_tuna\", \"icon_plate_shrimp\")}\n",
    "\n",
    "for entry_idx, entry in tqdm.tqdm(enumerate(ds.entries)):\n",
    "    for category_id in range(len(ds.thing_classes)):\n",
    "        cat_active_tracks = active_tracks[category_id]\n",
    "        bboxes_det, anno_idxs = entry.extract_bboxes(category_id)\n",
    "        bboxes_trk = [track[-1][2] for track in cat_active_tracks]\n",
    "        if len(bboxes_trk) > 0:\n",
    "            bboxes_trk = np.stack(bboxes_trk, axis=0)\n",
    "\n",
    "        min_iou = 0.0 if category_id not in iou_threshs else iou_threshs[category_id]\n",
    "        matched_det_trk_idx_pairs, unmatched_det_idxs, unmatched_trk_idxs = assign_bbox_matches(bboxes_det, bboxes_trk, min_iou=min_iou)\n",
    "        for det_idx, trk_idx in matched_det_trk_idx_pairs:\n",
    "            cat_active_tracks[trk_idx].append((entry_idx, anno_idxs[det_idx], bboxes_det[det_idx]))\n",
    "        for trk_idx in sorted(list(unmatched_trk_idxs), reverse=True):\n",
    "            cat_active_tracks.pop(trk_idx)\n",
    "        for det_idx in unmatched_det_idxs:\n",
    "            new_track = [(entry_idx, anno_idxs[det_idx], bboxes_det[det_idx])]\n",
    "            cat_active_tracks.append(new_track)\n",
    "            tracks[category_id].append(new_track)\n",
    "\n",
    "print()\n",
    "for cat_id, track in enumerate(tracks):\n",
    "    print(f\"category {cat_id} ({ds.thing_classes[cat_id]}): {len(track)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1642433081365,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "stH0SZra9qR9",
    "outputId": "f63bb82b-6bdf-44d3-c7dd-20567c505e0b"
   },
   "outputs": [],
   "source": [
    "### EXPLORE: print and visualize temporal tracks for given category\n",
    "\n",
    "cat_id = 9\n",
    "msg = f\"For category {cat_id} ({ds.thing_classes[cat_id]})\"\n",
    "print(msg)\n",
    "plt.figure()\n",
    "for track_idx, track in enumerate(tracks[cat_id]):\n",
    "    start_entry_idx = track[0][0]\n",
    "    end_entry_idx = track[-1][0]\n",
    "    print(f\"track idx {track_idx}: {len(track)} (idx=[{start_entry_idx} ... {end_entry_idx}]\")\n",
    "    plt.plot((start_entry_idx, end_entry_idx), (track_idx, track_idx), \"-ok\")\n",
    "plt.title(msg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5wq7wWPrfZE"
   },
   "outputs": [],
   "source": [
    "### VISUALIZE: for given category and track idx, draw frames of track\n",
    "\n",
    "cat_id, track_idx = 9, 0\n",
    "images = []  # list(tile_img, label, entry_id)\n",
    "for entry_idx, anno_idx, bbox_xyxy in tqdm.tqdm(tracks[cat_id][track_idx]):\n",
    "    entry = ds.entries[entry_idx]\n",
    "    img = cv2.cvtColor(cv2_imread(os.path.join(SOURCE_DIR, entry.file_name)), cv2.COLOR_BGR2RGB)\n",
    "    xmin, ymin, xmax, ymax = (int(v) for v in np.round(bbox_xyxy))\n",
    "    images.append((img[ymin:(ymax+1), xmin:(xmax+1)], anno_idx, entry_idx))\n",
    "plot_image_tiles(images, num_rows=5, num_cols=10, figsize=(28, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2048,
     "status": "ok",
     "timestamp": 1642433131189,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "D1ULsWuEt-Y9",
    "outputId": "9d6fae2d-6aa5-40d7-a1ba-153319dc7b7d"
   },
   "outputs": [],
   "source": [
    "### EXPLORE: test Kalman Filter's accuracy (xmid, ymid, area, aspect_ratio) between prediction and labels\n",
    "\n",
    "# NOTE: first use cells above to load ds (pred or labels), compute all tracks, and choose category and track\n",
    "\n",
    "cat_id, track_idx = 9, 0\n",
    "n_sigma = 2\n",
    "\n",
    "process_stdevs = (1, 1, 25, 0.02, 32, 24, 100)\n",
    "measurement_stdevs = (1, 1, 100, 0.02)\n",
    "\n",
    "min_iou_gt = 0.6\n",
    "ds_gt = DetectionDataset.loadFromJson(os.path.join(SOURCE_DIR, \"detection_dataset.json\"))\n",
    "\n",
    "residuals = {k: [] for k in (\"xmid\", \"ymid\", \"area\", \"ar\", \"entry_idx\")}  # residual = pred - gt\n",
    "debug = {k: [] for k in (\"entry_idx\", \"pred_bbox_xyxy\", \"gt_bbox_xyxy\", \"pred_x\", \"pred_P_diag\")}\n",
    "\n",
    "def get_stats(anno):\n",
    "    width = float(anno.right - anno.left + 1)\n",
    "    height = float(anno.bottom - anno.top + 1)\n",
    "\n",
    "    return {\n",
    "        \"xmid\": (anno.right + anno.left) / 2.0,\n",
    "        \"ymid\": (anno.bottom + anno.top) / 2.0,\n",
    "        \"area\": width * height,\n",
    "        \"ar\": width / height,\n",
    "    }\n",
    "\n",
    "kf = None\n",
    "for entry_idx, anno_idx, bbox_xyxy in tqdm.tqdm(tracks[cat_id][track_idx]):\n",
    "    if kf is None:\n",
    "        kf = ConstantVelocityBboxKalmanTracker(bbox_xyxy, process_stdevs=process_stdevs, measurement_stdevs=measurement_stdevs)\n",
    "        continue\n",
    "\n",
    "    pred_bbox_xyxy, pred_x = kf.predict()\n",
    "    pred_P = kf.P\n",
    "    # if entry_idx % 2 == 0:\n",
    "    kf.update(bbox_xyxy)\n",
    "\n",
    "    # Find closest ground-truth annotation\n",
    "    best_anno_gt, best_iou = None, min_iou_gt\n",
    "    for anno_gt in ds_gt.entries[entry_idx].annotations:\n",
    "        if anno_gt.category_id != cat_id:\n",
    "            continue\n",
    "        iou = ds.entries[entry_idx].annotations[anno_idx].iou(anno_gt)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_anno_gt = anno_gt\n",
    "    if anno_gt is None:\n",
    "        continue\n",
    "\n",
    "    # Collect residual\n",
    "    pred_stats = get_stats(BBoxAnnotation(*pred_bbox_xyxy, category_id=cat_id))\n",
    "    gt_stats = get_stats(best_anno_gt)\n",
    "    for k in residuals:\n",
    "        if k == \"entry_idx\":\n",
    "            residuals[k].append(entry_idx)  # hack\n",
    "        else:\n",
    "            residuals[k].append(pred_stats[k] - gt_stats[k])\n",
    "    \n",
    "    debug[\"entry_idx\"].append(entry_idx)\n",
    "    debug[\"pred_bbox_xyxy\"].append(pred_bbox_xyxy.flatten())\n",
    "    debug[\"gt_bbox_xyxy\"].append(best_anno_gt.toDict()[\"bbox\"])\n",
    "    debug[\"pred_x\"].append(pred_x)\n",
    "    debug[\"pred_P_diag\"].append(np.diag(pred_P))\n",
    "\n",
    "for k in residuals:\n",
    "    if k == \"entry_idx\":\n",
    "        continue\n",
    "    mean = np.mean(residuals[k])\n",
    "    sigma = np.std(residuals[k])\n",
    "    print(f\"residuals (pred - obs) for {k}: u={mean:.3f} / sigma={sigma:.3f}\")\n",
    "\n",
    "entry_idxs = residuals[\"entry_idx\"]\n",
    "gt_bbox_xyar = np.array([convert_bboxes_xyxy2xyar(np.array(bbox_xyxy)) for bbox_xyxy in debug[\"gt_bbox_xyxy\"]])\n",
    "pred_x = np.array(debug[\"pred_x\"])\n",
    "pred_bbox_xyar = pred_x[:, :4]\n",
    "pred_v_xya = pred_x[:, 4:]\n",
    "pred_P_diag = np.array(debug[\"pred_P_diag\"])\n",
    "pred_std_bbox_xyar = np.sqrt(pred_P_diag[:, :4])\n",
    "pred_std_v_xya = np.sqrt(pred_P_diag[:, 4:])\n",
    "\n",
    "labels = (\"xmid\", \"ymid\", \"area\", \"aspect_ratio\", \"vx\", \"vy\", \"v_area\")\n",
    "plt.figure(figsize=(24, 20))\n",
    "for plt_idx in range(7):\n",
    "  plt.subplot(4, 2, plt_idx + 1)\n",
    "  if plt_idx < 4:\n",
    "    pred_ys = pred_bbox_xyar[:, plt_idx]\n",
    "    pred_y_errors = n_sigma * pred_std_bbox_xyar[:, plt_idx]\n",
    "    plt.fill_between(entry_idxs, pred_ys - pred_y_errors, pred_ys + pred_y_errors, color=\"plum\")\n",
    "    plt.plot(entry_idxs, pred_ys, \"-\", color=\"purple\")\n",
    "    plt.plot(entry_idxs, gt_bbox_xyar[:, plt_idx], \"g-\")\n",
    "  else:\n",
    "    pred_ys = pred_v_xya[:, plt_idx-4]\n",
    "    pred_y_errors = n_sigma * pred_std_v_xya[:, plt_idx-4]\n",
    "    plt.fill_between(entry_idxs, pred_ys - pred_y_errors, pred_ys + pred_y_errors, color=\"plum\")\n",
    "    plt.plot(entry_idxs, pred_ys, \"-\", color=\"purple\")\n",
    "\n",
    "  axes = list(plt.axis())\n",
    "  axis_y_height = axes[3] - axes[2]\n",
    "  axis_y_mid = (axes[2] + axes[3]) / 2\n",
    "  if plt_idx < 4:\n",
    "    axis_y_height *= 0.4\n",
    "  else:\n",
    "    axis_y_height *= 0.6\n",
    "  axes[2] = axis_y_mid - axis_y_height / 2\n",
    "  axes[3] = axis_y_mid + axis_y_height / 2\n",
    "  plt.axis(axes)\n",
    "\n",
    "  plt.xlabel(labels[plt_idx])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91752,
     "status": "ok",
     "timestamp": 1642462933898,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "OPksagiPpOyF",
    "outputId": "ee2bd0ef-26c0-4503-ffd1-3bab3cef53fd"
   },
   "outputs": [],
   "source": [
    "### VISUALIZE: load dataset (gt/pred) and visualize tracks based on category and iou+dist proximity\n",
    "\n",
    "# ds = DetectionDataset.loadFromJson(os.path.join(SOURCE_DIR, \"detection_dataset.pred_2.json\"))\n",
    "ds = DetectionDataset.loadFromJson(os.path.join(eval_cfg.OUTPUT_DIR, \"detection_dataset.pred_2.json\"))\n",
    "\n",
    "kf_process_stdevs = (2, 2, 40, 0.02, 4, 3, 50)  # tuning for chefs\n",
    "kf_measurement_stdevs = (1, 1, 25, 0.01)\n",
    "\n",
    "max_num_kf_predicts = 5\n",
    "kf_category_ids = set(ds.thing_classes.index(cat) for cat in (\"chef\", \"chef_carrying\", \"chef_chopping\", \"plate\", \"shrimp\", \"tuna\", \"cut_shrimp\", \"cut_tuna\", \"plate_cut_shrimp\", \"plate_cut_tuna\", \"icon_shrimp\", \"icon_tuna\"))\n",
    "\n",
    "TrackEntry = namedtuple(\"TrackEntry\", (\"track_idx\", \"entry_idx\", \"anno_idx\", \"bbox_xyxy\", \"anno\", \"kf\"))\n",
    "tracks = [[] for _ in range(len(ds.thing_classes))]\n",
    "active_tracks = [[] for _ in range(len(ds.thing_classes))]\n",
    "free_track_idx = 0\n",
    "iou_threshs = {ds.thing_classes.index(category): 0.1 for category in (\"icon_plate_tuna\", \"icon_plate_shrimp\")}\n",
    "vid_fps = 5.0\n",
    "\n",
    "# for entry_idx, entry in enumerate(tqdm.tqdm(ds.entries)):  # DEBUG: uncomment below section to debug fn\n",
    "def make_frame(t):\n",
    "    global free_track_idx\n",
    "    entry_idx = int(t * vid_fps)\n",
    "    entry = ds.entries[entry_idx]\n",
    "\n",
    "    # Iterate over all categories: update tracks\n",
    "    for category_id in range(len(ds.thing_classes)):\n",
    "        # TODO: remove bypass to only track kf categories (i.e. fg)\n",
    "        if category_id not in kf_category_ids:\n",
    "          continue\n",
    "\n",
    "        use_hf = (category_id in kf_category_ids)\n",
    "        cat_active_tracks = active_tracks[category_id]\n",
    "        bboxes_det, anno_idxs = entry.extract_bboxes(category_id)\n",
    "        bboxes_trk = [track[-1].bbox_xyxy for track in cat_active_tracks]\n",
    "        if len(bboxes_trk) > 0:\n",
    "            bboxes_trk = np.stack(bboxes_trk, axis=0)\n",
    "\n",
    "        min_iou = 0.0 if category_id not in iou_threshs else iou_threshs[category_id]\n",
    "        matched_det_trk_idx_pairs, unmatched_det_idxs, unmatched_trk_idxs = assign_bbox_matches(bboxes_det, bboxes_trk, min_iou=min_iou)\n",
    "        for det_idx, trk_idx in matched_det_trk_idx_pairs:\n",
    "            kf = None\n",
    "            if use_hf:\n",
    "                kf = cat_active_tracks[trk_idx][-1].kf\n",
    "                kf.predict()  # NOTE: still need to predict to update KF's state and cov estimates\n",
    "                kf.update(bboxes_det[det_idx])\n",
    "            anno_idx = anno_idxs[det_idx]\n",
    "            cat_active_tracks[trk_idx].append(TrackEntry(\n",
    "                cat_active_tracks[trk_idx][-1].track_idx,\n",
    "                entry_idx,\n",
    "                anno_idx,\n",
    "                bboxes_det[det_idx],\n",
    "                entry.annotations[anno_idx],\n",
    "                kf)\n",
    "            )\n",
    "        for trk_idx in sorted(list(unmatched_trk_idxs), reverse=True):\n",
    "            kf = None\n",
    "            if use_hf:\n",
    "                kf = cat_active_tracks[trk_idx][-1].kf\n",
    "                if kf.num_conseq_predicts < max_num_kf_predicts:\n",
    "                    pred_bbox_xyxy, _ = kf.predict()\n",
    "                    cat_active_tracks[trk_idx].append(TrackEntry(\n",
    "                        cat_active_tracks[trk_idx][-1].track_idx,\n",
    "                        entry_idx,\n",
    "                        None,\n",
    "                        pred_bbox_xyxy,\n",
    "                        BBoxAnnotation(*pred_bbox_xyxy, category_id, score=-kf.num_conseq_predicts),\n",
    "                        kf)\n",
    "                    )\n",
    "                    continue\n",
    "                # else pop()\n",
    "            cat_active_tracks.pop(trk_idx)\n",
    "        for det_idx in unmatched_det_idxs:\n",
    "            kf = ConstantVelocityBboxKalmanTracker(bboxes_det[det_idx], process_stdevs=kf_process_stdevs, measurement_stdevs=kf_measurement_stdevs) if use_hf else None\n",
    "            anno_idx = anno_idxs[det_idx]\n",
    "            new_track = [TrackEntry(\n",
    "                free_track_idx,\n",
    "                entry_idx,\n",
    "                anno_idx,\n",
    "                bboxes_det[det_idx],\n",
    "                entry.annotations[anno_idx],\n",
    "                kf),\n",
    "            ]\n",
    "            free_track_idx += 1\n",
    "            cat_active_tracks.append(new_track)\n",
    "            tracks[category_id].append(new_track)\n",
    "      \n",
    "    # Generate and return frame\n",
    "    img = cv2_imread(os.path.join(SOURCE_DIR, entry.file_name))\n",
    "    track_entries = []\n",
    "    for cat_active_tracks in active_tracks:\n",
    "        for track in cat_active_tracks:\n",
    "            track_entries.append(track[-1])\n",
    "    track_entries.sort(key=lambda e: (e.anno.top, e.anno.bottom, e.anno.left, e.anno.right))\n",
    "    annotations = []\n",
    "    frame_track_idxs = []\n",
    "    for e in track_entries:\n",
    "        annotations.append(e.anno)\n",
    "        frame_track_idxs.append(e.track_idx)\n",
    "    img_annos = plot_track_annotations(img, annotations, frame_track_idxs, plot_labels)\n",
    "    return cv2.cvtColor(img_annos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "clip = mpy.VideoClip(make_frame, duration=len(ds.entries) / vid_fps - 1e-10)\n",
    "clip.write_videofile(\"KF_pred.mp4\", fps=vid_fps)\n",
    "\n",
    "for cat_id, track in enumerate(tracks):\n",
    "    print(f\"category {cat_id} ({ds.thing_classes[cat_id]}): {len(track)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1642462934356,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "thPmnXIK9OyB",
    "outputId": "be6c63e2-742a-483e-d65f-67cff101a1f9"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(\"KF_pred.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174388,
     "status": "ok",
     "timestamp": 1642740950330,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "NMqVHAk39MZO",
    "outputId": "db56b74b-5866-4ffd-8acb-7aab3a17543d"
   },
   "outputs": [],
   "source": [
    "### VISUALIZATION: Create video of homography from labelled bboxes vs imgproc-solved homography\n",
    "\n",
    "consensus_max_tile_pxs = 125\n",
    "vid_fps = 5.0\n",
    "\n",
    "ds = DetectionDataset.loadFromJson(os.path.join(SOURCE_DIR, \"detection_dataset.pred_2.json\"))\n",
    "tile_coords = get_tile_coords(tile_labels, tuple(ds.thing_classes))\n",
    "num_grid_rows, num_grid_cols = len(tile_labels), len(tile_labels[0])\n",
    "\n",
    "# grid: map grid_px = (tile_idx*num_pixels_per_tile+1:num_pixels_per_tile) to tile_idx\n",
    "# more technically: map num_pixels_per_tile/2 to 0, num_pixels_per_tile + num_pixels_per_tile/2 to 1, ...\n",
    "grid_coords = tile_coords.copy()\n",
    "grid_coords[:, :2] = grid_coords[:, :2] * num_pixels_per_tile + num_pixels_per_tile / 2\n",
    "grid_hxys_segments = []\n",
    "for row_idx in range(num_grid_rows + 1):\n",
    "    row_px = row_idx * num_pixels_per_tile\n",
    "    grid_hxys_segments.append((0, row_px, 1.0))\n",
    "    grid_hxys_segments.append((num_grid_cols * num_pixels_per_tile, row_px, 1.0))\n",
    "for col_idx in range(num_grid_cols + 1):\n",
    "    col_px = col_idx * num_pixels_per_tile\n",
    "    grid_hxys_segments.append((col_px, 0, 1.0))\n",
    "    grid_hxys_segments.append((col_px, num_grid_rows * num_pixels_per_tile, 1.0))\n",
    "grid_hxys_segments = np.array(grid_hxys_segments)\n",
    "\n",
    "def make_frame(t, label_margin_px=2, bbox_thickness=2, font_face=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.5, font_thickness=2):\n",
    "    img_idx = int(t * vid_fps)\n",
    "    bbox_half_thickness = bbox_thickness // 2\n",
    "\n",
    "    entry = ds.entries[img_idx]\n",
    "    det_coords = np.array([((anno.left + anno.right)/2, (anno.top + anno.bottom)/2, anno.category_id) for anno in entry.annotations])\n",
    "    H_img_grid, consensus_matched_det_grid_idxs, det_hxys, grid_hxys = compute_tile_homography(det_coords, grid_coords, consensus_max_tile_pxs)\n",
    "    H_img_grid_imgproc = np.linalg.inv(np.array(entry.H_grid_img).reshape((3, 3)))\n",
    "    img_hxys_segments = apply_homography(H_img_grid, grid_hxys_segments)\n",
    "    img_hxys_segments_imgproc = apply_homography(H_img_grid_imgproc, grid_hxys_segments)\n",
    "    img_frame = cv2.cvtColor(cv2.imread(os.path.join(SOURCE_DIR, ds.entries[img_idx].file_name)), cv2.COLOR_RGB2BGR)\n",
    "    img_height, img_width = img_frame.shape[:2]\n",
    "\n",
    "    for idx in range(0, len(img_hxys_segments_imgproc), 2):\n",
    "        x1, y1, x2, y2 = img_hxys_segments_imgproc[idx, 0], img_hxys_segments_imgproc[idx, 1], img_hxys_segments_imgproc[idx+1, 0], img_hxys_segments_imgproc[idx+1, 1]\n",
    "        cv2.line(img_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 5, cv2.LINE_AA)\n",
    "    for idx in range(0, len(img_hxys_segments), 2):\n",
    "        x1, y1, x2, y2 = img_hxys_segments[idx, 0], img_hxys_segments[idx, 1], img_hxys_segments[idx+1, 0], img_hxys_segments[idx+1, 1]\n",
    "        cv2.line(img_frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    for det_idx, grid_idx in consensus_matched_det_grid_idxs:\n",
    "        grid_col_idx, grid_row_idx = (int(idx) for idx in tile_coords[grid_idx, :2])\n",
    "        anno = entry.annotations[det_idx]\n",
    "        xmin, ymin, xmax, ymax = int(anno.left), int(anno.top), int(anno.right), int(anno.bottom)\n",
    "        xmid, ymid = (xmin + xmax) // 2, (ymin + ymax) // 2\n",
    "        cv2.rectangle(img_frame, (xmin, ymin), (xmax, ymax), (0, 0, 0), 5, cv2.LINE_AA)\n",
    "        cv2.rectangle(img_frame, (xmin, ymin), (xmax, ymax), (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    for det_idx, grid_idx in consensus_matched_det_grid_idxs:\n",
    "        grid_col_idx, grid_row_idx = (int(idx) for idx in tile_coords[grid_idx, :2])\n",
    "        category = tile_labels[grid_row_idx][grid_col_idx]\n",
    "        category = plot_labels[ds.thing_classes.index(category)]\n",
    "        anno = entry.annotations[det_idx]\n",
    "        xmin, ymin, xmax, ymax = int(anno.left), int(anno.top), int(anno.right), int(anno.bottom)\n",
    "        xmid, ymid = (xmin + xmax) // 2, (ymin + ymax) // 2\n",
    "\n",
    "        coord_str = f\"{category}({grid_row_idx},{grid_col_idx})\"\n",
    "        (coord_width, coord_height_wo_bl), coord_baseline = cv2.getTextSize(coord_str, font_face, font_scale, font_thickness)\n",
    "        coord_height = coord_height_wo_bl + coord_baseline\n",
    "        bbox_tl_px = (int(xmid - coord_width/2.0), int(ymid - coord_height/2.0))\n",
    "        coord_tl_px = (bbox_tl_px[0] - bbox_half_thickness, bbox_tl_px[1] - bbox_half_thickness)\n",
    "        coord_br_px = (bbox_tl_px[0] + coord_width + 2*label_margin_px - bbox_half_thickness, bbox_tl_px[1] + coord_height + 2*label_margin_px - bbox_half_thickness)\n",
    "        cv2.rectangle(img_frame, coord_tl_px, coord_br_px, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "    for det_idx, grid_idx in consensus_matched_det_grid_idxs:\n",
    "        grid_col_idx, grid_row_idx = (int(idx) for idx in tile_coords[grid_idx, :2])\n",
    "        category = tile_labels[grid_row_idx][grid_col_idx]\n",
    "        category = plot_labels[ds.thing_classes.index(category)]\n",
    "        anno = entry.annotations[det_idx]\n",
    "        xmin, ymin, xmax, ymax = int(anno.left), int(anno.top), int(anno.right), int(anno.bottom)\n",
    "        xmid, ymid = (xmin + xmax) // 2, (ymin + ymax) // 2\n",
    "\n",
    "        coord_str = f\"{category}({grid_row_idx},{grid_col_idx})\"\n",
    "        (coord_width, coord_height_wo_bl), coord_baseline = cv2.getTextSize(coord_str, font_face, font_scale, font_thickness)\n",
    "        coord_height = coord_height_wo_bl + coord_baseline\n",
    "        bbox_tl_px = (int(xmid - coord_width/2.0), int(ymid - coord_height/2.0))\n",
    "        coord_text_px = (bbox_tl_px[0] + label_margin_px - bbox_half_thickness, bbox_tl_px[1] + coord_height_wo_bl + label_margin_px - bbox_half_thickness)\n",
    "        cv2.putText(img_frame, coord_str, coord_text_px, font_face, font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n",
    "        \n",
    "    title_str = f\"{img_idx}: {entry.file_name} (green=H_imgproc, purple=H_autocorr)\"\n",
    "    (title_width, title_height_wo_bl), title_baseline = cv2.getTextSize(title_str, font_face, font_scale, font_thickness)\n",
    "    bbox_tl_px = (20, 20)\n",
    "    title_text_px = (bbox_tl_px[0] + label_margin_px - bbox_half_thickness, bbox_tl_px[1] + title_height_wo_bl + label_margin_px - bbox_half_thickness)\n",
    "    title_tl_px = (bbox_tl_px[0] - bbox_half_thickness, bbox_tl_px[1] - bbox_half_thickness)\n",
    "    title_br_px = (bbox_tl_px[0] + title_width + 2*label_margin_px - bbox_half_thickness, bbox_tl_px[1] + title_height_wo_bl + title_baseline + 2*label_margin_px - bbox_half_thickness)\n",
    "    cv2.rectangle(img_frame, title_tl_px, title_br_px, (255, 255, 255), cv2.FILLED)\n",
    "    cv2.putText(img_frame, title_str, title_text_px, font_face, font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n",
    "\n",
    "    return img_frame\n",
    "\n",
    "clip = mpy.VideoClip(make_frame, duration=len(ds.entries) / vid_fps - 1e-10)\n",
    "clip.write_videofile(\"H_autocorr.mp4\", fps=vid_fps)\n",
    "\n",
    "# Test plotting\n",
    "# plt.figure(figsize=(20, 15))\n",
    "# plt.imshow(make_frame(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1642740953742,
     "user": {
      "displayName": "Anqi Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjx5eTOnyn5JLftbbxa64v2skXZZpfyJ73ezrue4Q=s64",
      "userId": "05497358686828403223"
     },
     "user_tz": 300
    },
    "id": "dLsn6Ftc-yvl",
    "outputId": "0a796da1-e628-46bd-ba0e-2cc38ddda2ae"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(\"H_autocorr.mp4\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "train_overcooked_detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
