{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from overcooked_ai.dataset_types import DetectionDataset\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "SOURCE_DIR = Path(\"/home/mimic/Overcooked2_1-1_jpeg\")\n",
    "MODELS_DIR = Path(\"/home/mimic/objdet/overcooked_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.6 ; cuda:  cu124\n",
      "Mon Jun 16 09:59:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| 30%   32C    P5             29W /  320W |    2732MiB /  10240MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1155      G   /usr/lib/xorg/Xorg                      437MiB |\n",
      "|    0   N/A  N/A            1375      G   /usr/bin/gnome-shell                     55MiB |\n",
      "|    0   N/A  N/A            2518      G   /usr/lib/insync/insync                    3MiB |\n",
      "|    0   N/A  N/A            2960      G   /tmp/.mount_Joplin1gyEt0/joplin          42MiB |\n",
      "|    0   N/A  N/A            5129      G   /opt/google/chrome/chrome                 3MiB |\n",
      "|    0   N/A  N/A            5176      G   ...ersion=20250612-050034.702000         70MiB |\n",
      "|    0   N/A  N/A           35410      G   ...ess --variations-seed-version        344MiB |\n",
      "|    0   N/A  N/A           36695      C   ...e/mimic/venv/local/bin/python        858MiB |\n",
      "|    0   N/A  N/A           39826      C   ...e/mimic/venv/local/bin/python        820MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/16 09:59:47 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=35, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=136, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[06/16 09:59:47 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 645 images left.\n",
      "\u001b[32m[06/16 09:59:47 d2.data.build]: \u001b[0mDistribution of instances among all 34 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "| TileFreeSpace | 0            | TileCounter.. | 13185        |  TileTopPass  | 645          |\n",
      "| TileBottomP.. | 645          | TileTrashCh.. | 645          | TileCutting.. | 1681         |\n",
      "| TileDishRet.. | 119          | TileShrimpC.. | 634          | TileTunaCrate | 645          |\n",
      "|     Chef      | 456          | ChefChopping  | 494          | ChefCarrying  | 340          |\n",
      "|     Plate     | 946          |   RawShrimp   | 375          | ChoppedShrimp | 259          |\n",
      "| ChoppedShri.. | 566          |    RawTuna    | 319          |  ChoppedTuna  | 199          |\n",
      "| ChoppedTuna.. | 65           |  IconShrimp   | 1352         |   IconTuna    | 986          |\n",
      "|  ProgressBar  | 558          |  OrderShrimp  | 524          |   OrderTuna   | 736          |\n",
      "|    Digit0     | 1426         |    Digit1     | 463          |    Digit2     | 610          |\n",
      "|    Digit3     | 267          |    Digit4     | 372          |    Digit5     | 368          |\n",
      "|    Digit6     | 208          |    Digit7     | 71           |    Digit8     | 257          |\n",
      "|    Digit9     | 158          |               |              |               |              |\n",
      "|     total     | 30574        |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[06/16 09:59:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1080, sample_style='choice')]\n",
      "\u001b[32m[06/16 09:59:47 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[06/16 09:59:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[06/16 09:59:47 d2.data.common]: \u001b[0mSerializing 645 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/16 09:59:47 d2.data.common]: \u001b[0mSerialized dataset takes 9.21 MiB\n",
      "\u001b[32m[06/16 09:59:47 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[32m[06/16 09:59:47 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/mimic/objdet/overcooked_models/20250323_225544/model_final.pth ...\n",
      "\u001b[32m[06/16 09:59:47 d2.engine.hooks]: \u001b[0mLoading scheduler from state_dict ...\n"
     ]
    }
   ],
   "source": [
    "## Load dataset and existing detectron2 object detection model\n",
    "\n",
    "rng_seed = 1337\n",
    "training_subset_ratio = 0.75\n",
    "dataset_json_path = SOURCE_DIR / \"detection_dataset.mar2025.json\"\n",
    "\n",
    "dataset = DetectionDataset.load_from_json(dataset_json_path)\n",
    "\n",
    "# Update file_name to absolute paths\n",
    "for entry in dataset.entries:\n",
    "    entry.file_name = str(SOURCE_DIR / entry.file_name)\n",
    "\n",
    "# Configure dataset into catalog, merely for loading existing trainer\n",
    "dataset_dict = dataset.to_dict()[\"dataset_dict\"]\n",
    "thing_classes = dataset.thing_classes\n",
    "thing_colors = [(255, 255, 255) for _ in range(len(thing_classes))]\n",
    "\n",
    "# Split into train and val\n",
    "np.random.seed(rng_seed)\n",
    "np.random.shuffle(dataset_dict)\n",
    "num_training_entries = int(len(dataset_dict) * training_subset_ratio)\n",
    "train_dataset_dict = dataset_dict[:num_training_entries]\n",
    "val_dataset_dict = dataset_dict[num_training_entries:]\n",
    "\n",
    "for d in [\"train\", \"val\"]:\n",
    "    dataset_name = \"overcooked_\" + d\n",
    "    if dataset_name in DatasetCatalog:\n",
    "        DatasetCatalog.remove(dataset_name)\n",
    "        MetadataCatalog.remove(dataset_name)\n",
    "    DatasetCatalog.register(dataset_name, lambda d=d: train_dataset_dict if d == \"train\" else val_dataset_dict)\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=thing_classes, thing_colors=thing_colors)\n",
    "\n",
    "# Load existing model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(MODELS_DIR / \"20250323_225544\" / \"model.yaml\")\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame: 100%|██████████| 860/860 [00:48<00:00, 17.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embedding dataset to /home/mimic/objdet/overcooked_models/embedding_dataset.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Build embedding dataset\n",
    "\n",
    "rcnn = trainer.model\n",
    "\n",
    "embedding_dataset = []\n",
    "for entry_dict in tqdm.tqdm(dataset_dict, desc=\"frame\"):\n",
    "    entry_dict = copy.deepcopy(entry_dict)\n",
    "    image = cv2.imread(entry_dict[\"file_name\"], cv2.IMREAD_COLOR)  # detectron2 defaults to BGR\n",
    "    image_tensor = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "    entry_dict[\"image\"] = image_tensor\n",
    "\n",
    "    H_grid_frame = np.array(entry_dict[\"H_grid_img\"]).reshape(3, 3)\n",
    "    H_grid_frame /= H_grid_frame[2, 2]\n",
    "    H_grid_frame = torch.tensor(H_grid_frame, dtype=torch.float64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = rcnn.preprocess_image([entry_dict])\n",
    "        features = rcnn.backbone(images.tensor)\n",
    "\n",
    "        fpn_flattened_vectors = []\n",
    "        for key in features:\n",
    "            x = F.adaptive_avg_pool2d(features[key], (1, 1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            fpn_flattened_vectors.append(x)\n",
    "\n",
    "        feature_embedding = torch.cat(fpn_flattened_vectors, dim=1).squeeze(0)\n",
    "    \n",
    "    embedding_dataset.append({\n",
    "        \"file_name\": entry_dict[\"file_name\"],\n",
    "        \"feature_embedding\": feature_embedding.to(\"cpu\"),\n",
    "        \"H_grid_frame\": H_grid_frame.to(\"cpu\"),\n",
    "    })\n",
    "\n",
    "embedding_dataset_path = MODELS_DIR / \"embedding_dataset.pth\"\n",
    "torch.save(embedding_dataset, embedding_dataset_path)\n",
    "print(f\"Saved embedding dataset to {embedding_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.4178e+02, -1.7869e+02,  7.5200e+01],\n",
      "         [ 4.1452e+02,  4.7522e+01,  4.2581e+02],\n",
      "         [ 6.3797e-01,  3.1237e-01,  1.0000e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Build nn.Module to regress homography matrix from feature embedding\n",
    "\n",
    "from overcooked_ai.game_maps import world_1_1_tile_short_labels\n",
    "from overcooked_ai.type_conversions import get_world_1_1_corner_grid_xys\n",
    "\n",
    "\n",
    "class DifferentiableDLT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, target_xys: torch.Tensor, source_xys: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        @param target_xys: (B x N x 2) target points\n",
    "        @param source_xys: (B x N x 2) source points\n",
    "\n",
    "        @return H_target_source: (B x 3 x 3) homography matrix\n",
    "        \"\"\"\n",
    "        B = source_xys.shape[0]\n",
    "        assert source_xys.shape == target_xys.shape == (B, 4, 2)\n",
    "\n",
    "        # Extract individual coordinates, of dim [B, 4]\n",
    "        source_xs, source_ys = source_xys[:, :, 0], source_xys[:, :, 1]\n",
    "        target_xs, target_ys = target_xys[:, :, 0], target_xys[:, :, 1]\n",
    "        zeros = torch.zeros_like(target_xs)\n",
    "        ones = torch.ones_like(target_xs)\n",
    "\n",
    "        # Build the A matrix for each correspondence\n",
    "        A1 = torch.stack([zeros, zeros, zeros, -source_xs, -source_ys, -ones, target_ys * source_xs, target_ys * source_ys, target_ys], dim=2)\n",
    "        A2 = torch.stack([source_xs, source_ys, ones, zeros, zeros, zeros, -target_xs * source_xs, -target_xs * source_ys, -target_xs], dim=2)\n",
    "        A = torch.stack([A1, A2], dim=2).reshape(B, 8, 9)  # [B, 8, 9]\n",
    "\n",
    "        # Compute homography via SVD\n",
    "        _, _, V = torch.linalg.svd(A)  # [B, 9, 9]\n",
    "        H_target_source = V[:, -1, :].reshape(B, 3, 3)  # Take last row of V and reshape\n",
    "\n",
    "        # Normalize so that H[2,2] = 1\n",
    "        H_target_source = H_target_source / H_target_source[:, 2:3, 2:3]\n",
    "\n",
    "        return H_target_source\n",
    "\n",
    "\n",
    "class HomographyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # [1, 4, 2]\n",
    "        self.corner_grid_xys = torch.tensor(get_world_1_1_corner_grid_xys(), dtype=torch.float64).unsqueeze(0)\n",
    "\n",
    "        self.corner_frame_xys_regressor = nn.Sequential(\n",
    "            nn.Linear(1280, 320),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(320, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 8),\n",
    "        )\n",
    "        self.dlt = DifferentiableDLT()\n",
    "\n",
    "    def forward(self, feature_embedding: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        @param feature_embedding: (B, 1280)\n",
    "\n",
    "        @return predicted_image_xys: (B, 8)\n",
    "        \"\"\"\n",
    "        corners_frame_xys_vec = self.corner_frame_xys_regressor(feature_embedding)  # [B, 8]\n",
    "        corners_frame_xys = corners_frame_xys_vec.view(-1, 4, 2).to(torch.float64)  # [B, 4, 2]\n",
    "\n",
    "        corner_grid_xys = self.corner_grid_xys.to(feature_embedding.device)  # [1, 4, 2]\n",
    "        corner_grid_xys = corner_grid_xys.repeat(corners_frame_xys.shape[0], 1, 1)  # [B, 4, 2]\n",
    "        \n",
    "        # NOTE: predict H_grid_frame, to easily apply to frame xys (as predicted by this net and by objdet)\n",
    "        H_grid_frame = self.dlt(target_xys=corner_grid_xys, source_xys=corners_frame_xys)  # [B, 3, 3]\n",
    "\n",
    "        return {\n",
    "            \"corners_frame_xys\": corners_frame_xys,\n",
    "            \"H_grid_frame\": H_grid_frame,\n",
    "        }\n",
    "\n",
    "\n",
    "def batched_apply_homography(H_target_source: torch.Tensor, xys_source: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply homography to source points.\n",
    "\n",
    "    @param H_target_source: (batch_size x 3 x 3) homography matrix\n",
    "    @param xys_source: (batch_size x N x 2) source points\n",
    "\n",
    "    @return xys_target: (batch_size x N x 2) target points\n",
    "    \"\"\"\n",
    "\n",
    "    hxys_source = torch.cat([xys_source, torch.ones_like(xys_source[:, :, :1])], dim=2)  # [batch_size, num_pts, 3]\n",
    "    hxys_target = H_target_source @ hxys_source.transpose(1, 2)  # [batch_size, 3, num_pts]\n",
    "    xys_target = hxys_target.transpose(1, 2)[:, :, :2] / hxys_target.transpose(1, 2)[:, :, 2:3]  # [batch_size, num_pts, 2]\n",
    "\n",
    "    return xys_target\n",
    "\n",
    "# Test if module works\n",
    "h_net = HomographyNet().to(device)\n",
    "\n",
    "feature_embedding = embedding_dataset[0][\"feature_embedding\"].to(device)\n",
    "\n",
    "res = h_net(feature_embedding)\n",
    "corners_frame_xys = res[\"corners_frame_xys\"]\n",
    "H_grid_frame = res[\"H_grid_frame\"]\n",
    "print(H_grid_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT H_grid_frame:\n",
      " [[ 1.29720744e+00  3.49389082e-01 -6.41973130e+02]\n",
      " [-7.60501013e-06  1.79799546e+00 -2.65532313e+02]\n",
      " [-7.40649707e-06  5.60571219e-04  1.00000000e+00]]\n",
      "T_H_grid_frame\n",
      " tensor([[ 1.2972e+00,  3.4939e-01, -6.4197e+02],\n",
      "        [-7.6050e-06,  1.7980e+00, -2.6553e+02],\n",
      "        [-7.4065e-06,  5.6057e-04,  1.0000e+00]], dtype=torch.float64)\n",
      "diff w/ gt:\n",
      "[[[9.77662395e-13 2.69784195e-14 4.08704182e-10]\n",
      "  [4.98995376e-13 1.21680443e-13 2.89901436e-10]\n",
      "  [4.54703312e-14 1.28442287e-13 0.00000000e+00]]]\n",
      "T_pred_frame_xys\n",
      " tensor([[[ 489.1548,  178.1699],\n",
      "         [ 351.0563,  735.7790],\n",
      "         [1644.6209,  730.5700],\n",
      "         [1496.0181,  177.9636]]], dtype=torch.float64)\n",
      "diff w/ gt:\n",
      " [[[7.34985406e-11 4.51905180e-12]\n",
      "  [7.63287744e-09 4.27741043e-08]\n",
      "  [2.20211405e-08 1.07354481e-08]\n",
      "  [4.26791757e-08 9.96948302e-10]]]\n"
     ]
    }
   ],
   "source": [
    "## Test homography fitting logic against existing ground truth\n",
    "\n",
    "from overcooked_ai.grid_homography import apply_homography\n",
    "from overcooked_ai.type_conversions import get_world_1_1_corner_grid_xys\n",
    "\n",
    "entry_dict = dataset_dict[0]\n",
    "\n",
    "H_grid_frame = np.array(entry_dict[\"H_grid_img\"]).reshape(3, 3)\n",
    "H_grid_frame /= H_grid_frame[2, 2]\n",
    "\n",
    "print(\"GT H_grid_frame:\\n\", H_grid_frame)\n",
    "\n",
    "# Get some ground truth homography, and map it into frame xys\n",
    "corner_grid_xys = get_world_1_1_corner_grid_xys()\n",
    "corner_grid_hxys = np.concatenate([corner_grid_xys, np.ones((4, 1))], axis=1)\n",
    "\n",
    "corner_frame_hxys = apply_homography(np.linalg.inv(H_grid_frame), corner_grid_hxys)\n",
    "corner_frame_xys = corner_frame_hxys[:, :2]\n",
    "\n",
    "dlt = DifferentiableDLT()\n",
    "\n",
    "T_grid_xys = torch.tensor(corner_grid_xys, dtype=torch.float64).unsqueeze(0)\n",
    "T_frame_xys = torch.tensor(corner_frame_xys, dtype=torch.float64).unsqueeze(0)\n",
    "\n",
    "T_H_grid_frame = dlt(target_xys=T_grid_xys, source_xys=T_frame_xys)\n",
    "print(\"T_H_grid_frame\\n\", T_H_grid_frame[0])\n",
    "\n",
    "print(\"diff w/ gt:\")\n",
    "print(np.abs(T_H_grid_frame.cpu().detach().numpy() - H_grid_frame))\n",
    "\n",
    "T_pred_frame_xys = batched_apply_homography(torch.linalg.inv(T_H_grid_frame), T_grid_xys)\n",
    "print(\"T_pred_frame_xys\\n\", T_pred_frame_xys)\n",
    "print(\"diff w/ gt:\\n\", np.abs(T_pred_frame_xys.cpu().detach().numpy() - corner_frame_xys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1280])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build embeddings+homography dataset and dataloader\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embedding_dataset_path: Path):\n",
    "        self.embedding_dataset = torch.load(embedding_dataset_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedding_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        return self.embedding_dataset[idx]\n",
    "\n",
    "dataset = EmbeddingDataset(embedding_dataset_path)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [training_subset_ratio, 1 - training_subset_ratio])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "next(iter(train_dataloader))[\"feature_embedding\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9798772def3e45adbba668999a250376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134553/3543351521.py:63: UserWarning: Using a target size (torch.Size([1, 4, 2])) that is different to the input size (torch.Size([256, 4, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  reprojection_loss = F.mse_loss(corners_grid_xys_pred, corner_grid_xys)\n",
      "/tmp/ipykernel_134553/3543351521.py:63: UserWarning: Using a target size (torch.Size([1, 4, 2])) that is different to the input size (torch.Size([133, 4, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  reprojection_loss = F.mse_loss(corners_grid_xys_pred, corner_grid_xys)\n",
      "/tmp/ipykernel_134553/3543351521.py:26: UserWarning: Using a target size (torch.Size([1, 4, 2])) that is different to the input size (torch.Size([215, 4, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  reprojection_loss = F.mse_loss(corners_grid_xys_pred, corner_grid_xys)\n"
     ]
    }
   ],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "\n",
    "def eval_homography_net(\n",
    "        homography_net: HomographyNet,\n",
    "        dataloader: torch.utils.data.DataLoader) -> tuple[float, float]:\n",
    "    corner_grid_xys = homography_net.corner_grid_xys.to(device)\n",
    "\n",
    "    total_num_samples = 0\n",
    "    total_homography_l2_loss = 0.0\n",
    "    total_reprojection_loss = 0.0\n",
    "\n",
    "    homography_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            feature_embedding = batch[\"feature_embedding\"].to(device)\n",
    "            H_grid_frame_gt = batch[\"H_grid_frame\"].to(device)\n",
    "\n",
    "            result = homography_net(feature_embedding)\n",
    "            corners_frame_xys_pred = result[\"corners_frame_xys\"]\n",
    "            H_grid_frame_pred = result[\"H_grid_frame\"]\n",
    "\n",
    "            homography_l2_loss = F.mse_loss(H_grid_frame_pred, H_grid_frame_gt)\n",
    "            total_homography_l2_loss += homography_l2_loss.item() * len(batch)\n",
    "\n",
    "            corners_grid_xys_pred = batched_apply_homography(H_grid_frame_pred, corners_frame_xys_pred)\n",
    "            reprojection_loss = F.mse_loss(corners_grid_xys_pred, corner_grid_xys)\n",
    "            total_reprojection_loss += reprojection_loss.item() * len(batch)\n",
    "\n",
    "            total_num_samples += len(batch)\n",
    "\n",
    "        total_homography_l2_loss /= total_num_samples\n",
    "        total_reprojection_loss /= total_num_samples\n",
    "\n",
    "    homography_net.train()\n",
    "\n",
    "    return total_homography_l2_loss, total_reprojection_loss\n",
    "\n",
    "\n",
    "def train_homography_net(\n",
    "        homography_net: HomographyNet,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        train_dataloader: torch.utils.data.DataLoader,\n",
    "        val_dataloader: torch.utils.data.DataLoader,\n",
    "        num_epochs: int) -> None:\n",
    "    corner_grid_xys = homography_net.corner_grid_xys.to(device)\n",
    "\n",
    "    homography_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm.tqdm(range(num_epochs), desc=\"epoch\")\n",
    "    for epoch_idx in pbar:\n",
    "        for batch in train_dataloader:\n",
    "            feature_embedding = batch[\"feature_embedding\"].to(device)\n",
    "            H_grid_frame_gt = batch[\"H_grid_frame\"].to(device)\n",
    "\n",
    "            result = homography_net(feature_embedding)\n",
    "            corners_frame_xys_pred = result[\"corners_frame_xys\"]\n",
    "            H_grid_frame_pred = result[\"H_grid_frame\"]\n",
    "\n",
    "            homography_l2_loss = F.mse_loss(H_grid_frame_pred, H_grid_frame_gt)\n",
    "\n",
    "            corners_grid_xys_pred = batched_apply_homography(H_grid_frame_pred, corners_frame_xys_pred)\n",
    "            reprojection_loss = F.mse_loss(corners_grid_xys_pred, corner_grid_xys)\n",
    "\n",
    "            # TODO: tune hparam weights\n",
    "            loss = homography_l2_loss + reprojection_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_homography_l2_loss, val_reprojection_loss = eval_homography_net(homography_net, val_dataloader)\n",
    "        pbar.set_description(f\"val_H_l2: {val_homography_l2_loss:.4f}, val_reproj: {val_reprojection_loss:.4f}\")\n",
    "\n",
    "homography_net = HomographyNet().to(device)\n",
    "# TODO: tune lr\n",
    "optimizer = torch.optim.AdamW(homography_net.parameters(), lr=3e-4)\n",
    "train_homography_net(homography_net, optimizer, train_dataloader, val_dataloader, num_epochs=1000)\n",
    "# eval_homography_net(homography_net, val_dataloader)\n",
    "\n",
    "# TODO: why is H l2 loss so high and increasing during training? bad initialization?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
